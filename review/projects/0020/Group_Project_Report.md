Group Project Report
=============


##1. Experiment design:
###1) Goal:
Our goal of experiment is to test whether TCP Cubic is better than other TCP variances in stability. We run three flows 
of a high-speed TCP variant over a long-RTT network path and three flows of long-term TCP-SACK flows over a short-RTT 
path. These two paths share a bottleneck link, which is significantly less than link capacity. In this experiment, to 
see how stable different protocols become as the buffer space of the bottleneck router varied, we vary the buffer space
of the bottleneck router from 200% to 20% of the BDP of the bottleneck. The background TCP traffic is added to the bottleneck link.

###2) Parameters:
We will vary TCP variants and the buffer space of the bottleneck router from 200% to 20% of the bottleneck’s BDP.

###3) Metrics:
We will use throughput for showing its oscillation to verify that using Cubic can maintain relatively less oscillation,
i.e. the better stability.

###4) Data collection:
We will use iperf to measure throughputs. We can obtain the output files which are generated by the iperf. We don’t need
to modify the software or write a script.

###5) Analysis plan:
Coefficient of Variation (CoV) is defined as the ratio of the standard deviation to the mean. In our experiment, we use 
CoV of throughput to analyze the stability.

##2.  Implementation of experiment design
We run 3 flows of a high-speed TCP variant over a long-RTT network path (~220ms) and 3 flows of long-term TCP-SACK flows
over a short-RTT path (~20ms). These two paths share a bottleneck link of 25Mbps. In this experiment, to see how stable 
different protocols become as the buffer space of the bottleneck router varied, we vary the buffer space of the bottleneck
router from 200% to 20% of the BDP of the bottleneck. The background TCP traffic is added to the bottleneck link. We get the
throughput data of each TCP variant by using iperf.

Then we use R to analyze throughput data and draw graphs of the throughputs of each TCP variant.

We also calculate the CoV (Coefficient of Variation) to depict stability when some perturbations to the traffic are added to 
the network. However, since network environments constantly change, the transmission rate of a protocol always fluctuates at
a short-term scale. Then, we vary the time scale to get CoV to determine its stability. 

##3. Results:
###1) Throughput:
####A) Buffer size = 20% of the BDP:
#####CUBIC:
![CUBIC buffer20](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/CUBIC-buffer20.png?token=aa99e8d13d4460f54465caaf6d1a8c67ab6a2935)

#####BIC:
![BIC buffer20](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/BIC-buffer20.png?token=103f1e54b99d68635524b7290583ce9111477b43)


#####HSTCP:
![HSTCP buffer20](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/HSTCP-buffer20.png?token=6df67e6922d17be89dfd1d68ecddf09c6cfa85a3)


#####HTCP:
![HTCP buffer20](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/HTCP-buffer20.png?token=8635fd4cc17b6fc12a81749cc6f90b7cac6b831c)


#####STCP:
![STCP buffer20](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/STCP-buffer20.png?token=ba1556442d3788d37a0cfcc5d11293bf1458f0cf)

####B) Buffer size = 200% of the BDP:

#####CUBIC:
![CUBIC buffer200](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/CUBIC-buffer200.png?token=755d9a2cb3f95966130f48ccb87edc8aa4f00e5d)

#####BIC:
![BIC buffer200](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/BIC-buffer200.png?token=b84ea2e694e4b2e7affc556534d93f774ef5f2e3)


#####HSTCP:
![HSTCP buffer200](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/HSTCP-buffer200.png?token=360932e83ffbd7aee4fb3c8dd59b53da7133d51a)


#####HTCP:
![HTCP buffer200](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/HTCP-buffer200.png?token=c0748d2e3733bebc47901032eea1189a2e74cafd)


#####STCP:
![STCP buffer200](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/4_17_image/STCP-buffer200.png?token=fad19b06befccc442694d5e09e5c875da32a53e6)

We find that using CUBIC can make the throughput curve more smooth and less oscillatory than other TCP variants in both buffer 
of 20% BDP and 200% BDP. This result shows that CUBIC can maintain a more stable performance than other TCP variants we used.

###2) Coefficient of Variation:

####A) Buffer size = 20% of the BDP:
![CoV buffer20](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/cov20.png?token=fbd0dd15f1b59b879ec9a859ec0cca98eb211d22)


####B) Buffer size = 200% of the BDP:
![CoV buffer200](https://bytebucket.org/unicorn319/cubic-stability-project/raw/099432f18be027d2392dbd7b2a5066018ad6240c/sample_data_and_image/cov200.png?token=572f9959d3a21804413951aad7e3cf9eae96ec09)


In the CoV approach, when buffer space of bottleneck is 20% of the BDP, the resulting graph shows that CUBIC is the one with 
the smallest CoV. i.e. CUBIC shows a good stability. However, when buffer space of bottleneck is 200% of the BDP, the graph 
shows that CUBIC has the highest CoV of all the TCP variants. This result is different from the paper that we read. We come up 
with several possible reasons of this difference:

* Because GENI is unable to accommodate the requests of setting capacity to 10Gbps, we reduced the capacity of switches to one
 hundredth of the value in paper. So in our experiment, the capacity of the bottleneck link and the other links are 25Mbps and
 100Mbps.
 
* We thought that the shrinkage of capacity cannot provide a good environment for 4 flows, so we only ran 3 flows in each trial.

##4. Instructions for reproducing this experiment:

We provided **a  Step-by-Step file** in this folder to guide you complete the whole process of this experiment. Please read this file carefully. 

A "Procedure of Repeating This Experiment"
file is also provided as an outline to make the you clearly understand the procedure of this experiment.
